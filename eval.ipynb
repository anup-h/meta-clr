{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from utils.utils import neq_load_customized\n",
    "from model.pretrain import InfoNCE\n",
    "from model.classifier import LinearClassifier\n",
    "import utils.transforms as T\n",
    "from utils.utils import calc_topk_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "normal-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalSupervised(pl.LightningModule):\n",
    "    def __init__(self, unsupervised_path):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.feat_ext = LinearClassifier(\n",
    "                    network='s3d', \n",
    "                    num_class=101,\n",
    "                    dropout=0.9,\n",
    "                    use_dropout=True,\n",
    "                    use_final_bn=False,\n",
    "                    use_l2_norm=False)\n",
    "        checkpoint = torch.load(unsupervised_path)\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        new_dict = {}\n",
    "        for k,v in state_dict.items():\n",
    "            k = k.replace('encoder_q.0.', 'backbone.')\n",
    "            new_dict[k] = v\n",
    "        state_dict = new_dict\n",
    "        neq_load_customized(self.feat_ext, state_dict, verbose=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts = transforms.Compose([\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], channel=1)])\n",
    "        return self.feat_ext(ts(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, _ = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, _ = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        top1, top5 = calc_topk_accuracy(y_hat, y, (1,5))\n",
    "        tensorboard_logs = {'val_loss': loss, 'top_1': top1, 'top_5': top5}\n",
    "        return {'val_loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressed-shaft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier to 101 classes with s3d backbone; + dropout 0.900000\n"
     ]
    }
   ],
   "source": [
    "m = EvalSupervised('CoCLR-ucf101-rgb-128-s3d-ep182.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59635f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innovative-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    lstv = []\n",
    "    lstl = []\n",
    "    for v, _, l in batch:\n",
    "        lstv.append(F.interpolate(torch.as_tensor(v/255.0).permute(3, 0, 1, 2).unsqueeze(0), size=(32, 128, 128), mode='trilinear').squeeze(0))\n",
    "        lstl.append(l)\n",
    "    return torch.stack(lstv), torch.as_tensor(lstl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satellite-retrieval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abbc3e96ca5414da2f3ccc22cb587c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torchvision/datasets/video_utils.py:215: UserWarning: There aren't enough frames in the current video to get a clip for the given clip length and frames between clips. The video (and potentially others) will be skipped.\n",
      "  warnings.warn(\"There aren't enough frames in the current video to get a clip for the given clip length and \"\n"
     ]
    }
   ],
   "source": [
    "ds = torchvision.datasets.UCF101('.//UCF-101', './ucfTrainTestlist', 32, train=True)\n",
    "# dl = DataLoader(ds, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c0e0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('.//UCF-101/'+os.listdir('.//UCF-101')[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "given-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337988e899f04c359051dca11d4f7f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vds = torchvision.datasets.UCF101('./UCF-101', './ucfTrainTestlist', 32, train=False)\n",
    "# vdl = DataLoader(ds, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "trained-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "vdl = DataLoader(ds, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer = pl.Trainer(gpus=2, progress_bar_refresh_rate=20, max_epochs=10, accelerator='ddp')\n",
    "trainer.fit(m, dl, vdl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-barrel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
